  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2020-10-08 04:18:26,863] {__init__.py:50} INFO - Using executor SequentialExecutor
[2020-10-08 04:18:26,868] {scheduler_job.py:1367} INFO - Starting the scheduler
[2020-10-08 04:18:26,869] {scheduler_job.py:1375} INFO - Running execute loop for -1 seconds
[2020-10-08 04:18:26,869] {scheduler_job.py:1376} INFO - Processing each file at most -1 times
[2020-10-08 04:18:26,869] {scheduler_job.py:1379} INFO - Searching for files in /home/ubuntu/eCommerce/airflow/dags
[2020-10-08 04:18:26,870] {scheduler_job.py:1381} INFO - There are 2 files in /home/ubuntu/eCommerce/airflow/dags
[2020-10-08 04:18:26,870] {scheduler_job.py:1438} INFO - Resetting orphaned tasks for active dag runs
[2020-10-08 04:18:26,893] {base_job.py:313} INFO - Reset the following 13 TaskInstances:
	<TaskInstance: s3_key_sensor_dag.print_message 2020-10-07 00:00:10+00:00 [None]>
	<TaskInstance: s3_key_sensor_dag.print_message 2020-10-07 00:00:11+00:00 [None]>
	<TaskInstance: s3_key_sensor_dag.print_message 2020-10-07 00:00:12+00:00 [None]>
	<TaskInstance: s3_key_sensor_dag.print_message 2020-10-07 00:00:13+00:00 [None]>
	<TaskInstance: s3_key_sensor_dag.print_message 2020-10-07 00:00:14+00:00 [None]>
	<TaskInstance: s3_key_sensor_dag.s3_key_sensor_task 2020-10-08 02:44:04.171716+00:00 [None]>
	<TaskInstance: s3_key_sensor_dag.s3_key_sensor_task 2020-10-07 00:00:18+00:00 [None]>
	<TaskInstance: s3_key_sensor_dag.s3_key_sensor_task 2020-10-07 00:00:21+00:00 [None]>
	<TaskInstance: s3_key_sensor_dag.s3_key_sensor_task 2020-10-07 00:00:16+00:00 [None]>
	<TaskInstance: s3_key_sensor_dag.s3_key_sensor_task 2020-10-07 00:00:19+00:00 [None]>
	<TaskInstance: s3_key_sensor_dag.print_message 2020-10-07 00:00:15+00:00 [None]>
	<TaskInstance: s3_key_sensor_dag.s3_key_sensor_task 2020-10-07 00:00:17+00:00 [None]>
	<TaskInstance: s3_key_sensor_dag.s3_key_sensor_task 2020-10-07 00:00:20+00:00 [None]>
[2020-10-08 04:18:26,897] {dag_processing.py:562} INFO - Launched DagFileProcessorManager with pid: 11044
[2020-10-08 04:18:26,900] {settings.py:55} INFO - Configured default timezone <Timezone [UTC]>
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[2020-10-08 04:18:41,492] {__init__.py:50} INFO - Using executor SequentialExecutor
[2020-10-08 04:18:41,493] {dagbag.py:417} INFO - Filling up the DagBag from /home/ubuntu/eCommerce/airflow/dags
Running the Gunicorn Server with:
Workers: 4 sync
Host: 0.0.0.0:8080
Timeout: 120
Logfiles: - -
=================================================================            
[2020-10-08 04:18:42 +0000] [11113] [INFO] Starting gunicorn 20.0.4

Error: Already running on PID 19436 (or pid file '/home/ubuntu/airflow/airflow-webserver.pid' is stale)

[2020-10-08 04:19:24,977] {scheduler_job.py:963} INFO - 2 tasks up for execution:
	<TaskInstance: s3_key_trigger.new_csv_sensor 2020-10-07 00:00:00+00:00 [scheduled]>
	<TaskInstance: s3_key_trigger.new_csv_sensor 2020-10-07 00:01:00+00:00 [scheduled]>
[2020-10-08 04:19:24,983] {scheduler_job.py:997} INFO - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued
[2020-10-08 04:19:24,984] {scheduler_job.py:1025} INFO - DAG s3_key_trigger has 0/16 running and queued tasks
[2020-10-08 04:19:24,984] {scheduler_job.py:1025} INFO - DAG s3_key_trigger has 1/16 running and queued tasks
[2020-10-08 04:19:24,988] {scheduler_job.py:1085} INFO - Setting the following tasks to queued state:
	<TaskInstance: s3_key_trigger.new_csv_sensor 2020-10-07 00:00:00+00:00 [scheduled]>
	<TaskInstance: s3_key_trigger.new_csv_sensor 2020-10-07 00:01:00+00:00 [scheduled]>
[2020-10-08 04:19:24,996] {scheduler_job.py:1159} INFO - Setting the following 2 tasks to queued state:
	<TaskInstance: s3_key_trigger.new_csv_sensor 2020-10-07 00:00:00+00:00 [queued]>
	<TaskInstance: s3_key_trigger.new_csv_sensor 2020-10-07 00:01:00+00:00 [queued]>
[2020-10-08 04:19:24,996] {scheduler_job.py:1195} INFO - Sending ('s3_key_trigger', 'new_csv_sensor', datetime.datetime(2020, 10, 7, 0, 0, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-10-08 04:19:24,996] {base_executor.py:58} INFO - Adding to queue: ['airflow', 'run', 's3_key_trigger', 'new_csv_sensor', '2020-10-07T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/ubuntu/eCommerce/airflow/dags/s3_triggered_spark_dag.py']
[2020-10-08 04:19:24,997] {scheduler_job.py:1195} INFO - Sending ('s3_key_trigger', 'new_csv_sensor', datetime.datetime(2020, 10, 7, 0, 1, tzinfo=<TimezoneInfo [UTC, GMT, +00:00:00, STD]>), 1) to executor with priority 3 and queue default
[2020-10-08 04:19:24,997] {base_executor.py:58} INFO - Adding to queue: ['airflow', 'run', 's3_key_trigger', 'new_csv_sensor', '2020-10-07T00:01:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/ubuntu/eCommerce/airflow/dags/s3_triggered_spark_dag.py']
[2020-10-08 04:19:24,997] {sequential_executor.py:54} INFO - Executing command: ['airflow', 'run', 's3_key_trigger', 'new_csv_sensor', '2020-10-07T00:00:00+00:00', '--local', '--pool', 'default_pool', '-sd', '/home/ubuntu/eCommerce/airflow/dags/s3_triggered_spark_dag.py']
[2020-10-08 04:19:26,947] {__init__.py:50} INFO - Using executor SequentialExecutor
[2020-10-08 04:19:26,948] {dagbag.py:417} INFO - Filling up the DagBag from /home/ubuntu/eCommerce/airflow/dags/s3_triggered_spark_dag.py
Running %s on host %s <TaskInstance: s3_key_trigger.new_csv_sensor 2020-10-07T00:00:00+00:00 [queued]> ip-10-0-0-6.us-east-2.compute.internal
